\documentclass[12pt,letterpaper,onecolumn,oneside]{article}
\usepackage[margin=3cm]{geometry}
\usepackage{graphicx}
\usepackage[colorlinks=true,linkcolor=blue]{hyperref}
\usepackage[page]{appendix}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbold}
\usepackage{filecontents}
\usepackage[round,nonamebreak]{natbib}
\usepackage{mathtools}
\usepackage{url}
\usepackage{siunitx}
\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}

%% I am embedding the bibliography in the latex file for simplicity !!

\begin{filecontents}{seismic.bib}
@misc{iaspei2011,
author ={{I}nternational {S}eismological {C}entre},
title = {{IASPEI} Standard Phase List}, 
url = {http://www.isc.ac.uk/standards/phases/},
publisher = {Internatl. Seis. Cent.}, 
address = {Thatcham, United Kingdom},
year={2011},
howpublished = {\url{http://www.isc.ac.uk/standards/phases/}}
}

@article{Arora2013,
  author = {Nimar S. Arora and Stuart Russell and Erik Sudderth},
  year = {2013},
  month = {April},
  title = {{NET-VISA}: Network Processing Vertically Integrated Seismic 
    Analysis},
  journal = {Bulletin of the Seismological Society of America},
  volume = {103 no. 2A},
  pages = {709--729}
}

@misc{ wiki-great-circle-dist,
   author = "Wikipedia",
   title = "{G}reat-circle distance",
   year = "2015",
   howpublished = "\url{http://en.wikipedia.org/wiki/Great-circle_distance}",
   note = "[Online; accessed 3-March-2015]"
 }

@misc{ wiki-gutenberg-richter-law,
   author = "Wikipedia",
   title = "{G}utenberg-{R}ichter law",
   year = "2015",
   howpublished = "\url{http://en.wikipedia.org/wiki/Gutenberg-Richter_law}",
   note = "[Online; accessed 3-March-2015]"
 }

@misc{ wiki-atan2,
   author = "Wikipedia",
   title = "atan2",
   year = "2015",
   howpublished = "\url{http://en.wikipedia.org/wiki/Atan2}",
   note = "[Online; accessed 3-March-2015]"
 }

@misc{ wiki-great-circle-navigation,
   author = "Wikipedia",
   title = "{G}reat-circle navigation",
   year = "2015",
   howpublished = "\url{http://en.wikipedia.org/wiki/Great-circle_navigation}",
   note = "[Online; accessed 3-March-2015]"
 }

}
\end{filecontents}

\begin{document}

\title{Seismic 2-D}
\author{
\textit{Nimar S. Arora}\\
Bayesian Logic, Inc.\\
Union City, CA 94587
\and
\textit{Stuart Russell}\\
Dept. of Computer Science\\
Berkeley, CA 94720
}
\maketitle

\begin{abstract}
The goal of this problem is to detect and localize seismic events on a
simulated two-dimensional world (the surface of a perfect sphere) given
signals collected over a fixed time interval at a number of seismic
stations. This is a simplification of the real problem
\citep{Arora2013}, and is designed as a challenge problem for
research in probabilistic programming languages.
\end{abstract}

\section{Generative Model}

\subsection{Events}

Events are generated by a homogenous space-time Poisson process over the
surface of the earth with rate parameter $\lambda_e$. We assume that the
earth is a perfect sphere with radius $R$, and we are only interested in
events that occur in an episode of length $T$ in time. In other words,
the number of events in any episode has a Poisson distribution with rate
$\lambda_e 4 \pi R^2 T$.  Further, it follows that the time of each
event is uniformly distributed in $[0,T]$, and the location is uniformly
distributed over the surface of the earth. If locations are represented
by longitude and latitude then one can equivalently state that
longitudes are uniformly distributed over $[-180, 180]$ and the $sin$ of
the latitude is uniformly distributed over $[-1, 1]$.

More formally, if $e$ is the set of events in an episode, then
\begin{align*}
  |e| & \sim \text{Poisson}(\ \cdot\ |\ \lambda_e 4 \pi R^2 T), \\
\intertext{and for each event $e^i$ with time, longitude, and latitude
  given by $e^i_t$, $e^i_{l1}$, and $e^i_{l2}$ respectively,}
  e^i_t & \sim \text{Uniform}(\ \cdot\ |\ 0, \ T) \\
  e^i_{l1} & \sim \text{Uniform}(\ \cdot\ |\ -180, \ 180) \\
  \sin(e^i_{l2}) & \sim \text{Uniform}(\ \cdot\ |\ -1, \ 1).
\intertext{The event magnitude, $e^i_m$ is distributed as per an
  exponential distribution with scale $\theta_m$, a
  minimum value of $\mu_m$ and a maximum of $\gamma_m$, i.e.,}
  e^i_m & \sim \text{Exponential}(\ \cdot \ | \ \mu_m, \ \theta_m, \ \gamma_m).
\intertext{This magnitude model is based on the well known
  Gutenberg-Richter law \citep{wiki-gutenberg-richter-law}. The maximum
  magnitude is a result of a phenomenon known as magnitude saturation.}
\end{align*}

\subsection{True Detections}

The seismic energy from an event travels radially outwards in distinct
phases, each of which may or may not be detected by a station depending
on local noise levels. For example, the energy can travel via different
modes of transmission (compression waves, shear waves) and along
different layers of the earth, the so called {\em body} waves or along
the surface, refer to \citet{iaspei2011} for a full list. In this work we only
consider the first arriving phase, the {\em P} phase.

We define a true detection $\Lambda^{ik}$ as the moment of first arrival
of the energy from an event $i$ at a seismic station $k$. Various signal
processing algorithms are applied to the raw waveforms to detect an
arrival, and then station processing algorithms collect various
attributes of the
detection such as time, azimuth, slowness, and amplitude referenced by
$\Lambda^{ik}_t$, $\Lambda^{ik}_z$, $\Lambda^{ik}_s$, and
$\Lambda^{ik}_a$ respectively. Time is quite obviously the detection time
of the energy, azimuth refers to the geographical direction of the
incoming seismic waves, and amplitude is the height of the initial
peak. Slowness is a more peculiar term, it refers to the inverse of the
apparent surface speed of the waves, which will become clearer shortly.

\subsubsection{Detection Probability}

The probability that an event $e^i$ is detected at station $s^k$ is a
function of the event magnitude and the great-circle distance,
$\Delta_{ik}$, between the event and the station. If $\Lambda^{ik}$ =
$\zeta$ represents a {\em mis}-detection then,
\[P(\Lambda^{ik} \ne \zeta \ | \ e^i , s^k) = \text{logistic}
(\mu_{d0}^k + \mu_{d1}^k e^i_m + \mu_{d2}^k \Delta_{ik}) \ .\]

\subsubsection{Detection Time}

The theoretical travel time of a seismic wave at a distance of $\delta$ is
given by the travel time function,
\[ I_T(\delta) = -.023 \times \delta^2 + 10.7 \times \delta\, + 5.\]
The detection time has a Laplacian distribution
 centered near the theoretical detection time,
\[ \Lambda_t^{ik} \sim \text{Laplacian}(\ \cdot \ | \ e^i_t + I_T(\Delta_{ik}) +
\mu_t^k \  , \ \theta_t^k) . \]
Note that the detections with time greater than $T$ will not be
available in the episodic data.

\subsection{Detection Azimuth}

The difference of the detection azimuth, $\Lambda_z^{ik}$ from the theoretical
station-to-event azimuth, $G_z(s^k_l, e^i_l)$  is distributed as a Laplacian,
\[\psi(G_z(s^k_l, e^i_l), \Lambda_z^{ik}) \sim \text{Laplacian}(\ \cdot
\ | \  \mu_z^k, \ \theta_z^k \ ) . \]
Here $s^k_l$ and $e^i_l$ refer to the locations (pair of longitude and
latiude) of the station and the event respectively. See
Appendix~\ref{app-sphere} for a definition of $G_z$ and $\psi$.

\subsection{Detection Slowness}
The slowness at distance $\delta$ given by $I_S(\delta)$ is simply the
derivative of the travel time. In other words, slowness measures
the time that the seismic wave takes to travel between two points very
close to the station in the direction of the event-to-station
azimuth. The theoretical slowness is given by the formula,
\[ I_S(\delta) = -.046 \times \delta + 10.7\, .\]
Note that $I_S$ is always positive since $\delta \in [0, 180]$.

The detection slowness is a Laplacian centered near the theoretical
slowness,

\[ \Lambda_s^{ik} \sim \text{Laplacian}(\ \cdot \ | \ I_S(\Delta_{ik}) +
\mu_s^k \  , \ \theta_s^k) . \]

\subsection{Detection Amplitude}

The log of the detection amplitude has a Gaussian distribution with a mean
determined by the event magnitude and travel time.

\[\log(\Lambda_a^{ik}) \sim \text{Gaussian}(\ \cdot \ | \ \mu^k_{a0} 
+ \mu^k_{a1} e^i_m + \mu^k_{a2} I_T(\Delta_{ik})\  ,\ \sigma_a^k \ ) . \]


\subsection{False Detections}

Each station $k$ has its own time-homogenous Poisson process generating
false detections with rate $\lambda^k_f$. In other words, if $\xi^k$ is
the set of false detections in an episode,
\begin{align*}
|\xi^k| & \sim \text{Poisson}(\ \cdot \ | \ \lambda^k_f \, T),
\intertext{and the detection time $\xi^k_t$ is uniformly distributed,}
\xi^k_t & \sim \text{Uniform}(\ \cdot \ | \ 0, \ T) .
\intertext{The azimuth and slowness are also uniformly distributed
  with their allowed ranges as follows:}
\xi^k_z & \sim \text{Uniform}(\ \cdot \ | \ 0, \ 360),  \\
\xi^k_s & \sim \text{Uniform}(\ \cdot \ | \ I_S(180), \ I_S(0)) .
\intertext{However, the log-amplitude is distributed as Cauchy,}
\log{\xi^k_a} & \sim \text{Cauchy}(\ \cdot \ | \ \mu^k_f, \ \theta^k_f) .
\end{align*}

\section{Hyperpriors and Constants}

\label{sec-hyperprior}

\begin{align*}
T & = 3600 \, s \\
R & = 6371 \, km \\
\lambda_e & \sim \text{Gamma}(\ \cdot \ | \ 6.0, \frac{1}{4 \pi R^2 T}) \\
\mu_m &= 3.0 \\
\theta_m & = 4.0 \\
\gamma_m &= 6.0 \\
\left[ 
  \begin{array}{l}
    \mu^k_{d0} \\
    \mu^k_{d1} \\
    \mu^k_{d2} 
  \end{array} \right] & = \text{MVarGaussian} \left( \ \cdot \ \left| \
\left[
  \begin{array}{S[table-format=3.4]}
    -10.4 \\
    3.26 \\
    -0.0499
  \end{array}
\right]
 , 
\left[
  \begin{array}{S[table-format=2.4]S[table-format=1.6]S[table-format=1.6]}
    13.43 & -2.36 & -.0122 \\
    -2.36 & .452 & .000112 \\
    -.0122 & .000112 & .000125 \\
  \end{array}
\right]
\ \right. \right) \\
\mu_t^k & = 0 \\
\theta_t^k & \sim \text{InvGamma}(\ \cdot \ | \ 120, \ 118) \\
\mu_z^k & = 0 \\
\theta_z^k & \sim \text{InvGamma}(\ \cdot \ | \ 5.2, \ 44) \\
\mu_s^k & = 0 \\
\theta_s^k & \sim \text{InvGamma}(\ \cdot \ | \ 6.7, \ 7.5) \\
\left[ 
  \begin{array}{l}
    \mu^k_{a0} \\
    \mu^k_{a1} \\
    \mu^k_{a2} 
  \end{array} \right] & = \text{MVarGaussian} \left( \ \cdot \ \left| \
\left[
  \begin{array}{S[table-format=2.5]}
    -7.3 \\
    2.03 \\
    -.00196
  \end{array}
\right]
 , 
\left[
  \begin{array}{S[table-format=2.6]S[table-format=1.7]S[table-format=1.9]}
    1.23 & -.227 & -.000175 \\
    -.227 & .0461 & .0000245 \\
    -.000175 & .0000245 & .000000302 \\
  \end{array}
\right]
\ \right. \right) \\
(\sigma^k_a)^2 & \sim \text{InvGamma}(\ \cdot \ | \  21.1, \ 12.6) \\
\lambda^k_f & \sim \text{Gamma}(\ \cdot \ | \  2.1, \ 0.0013) \\
\mu^k_f & \sim \text{Gaussian}(\ \cdot \ | \  -0.68, \ 0.68) \\
\theta^k_f & \sim \text{InvGamma}(\ \cdot \ | \  23.5, \ 12.45) \\
\end{align*}

\section{Stations}

\begin{tabular}{lrrr}
Abbreviation & Station Number & Longitude & Latitude \\
ASAR & 0 & 133.9 & -23.7 \\
CMAR & 1 & 98.9 & 18.5 \\
FINES & 2 & 26.1 & 61.4 \\
ILAR & 3 & -146.9 & 64.8 \\
MKAR & 4 & 82.3 & 46.8 \\
SONM & 5 & 106.4 & 47.8 \\
STKA & 6 & 141.6 & -31.9 \\
TORD & 7 & 1.7 & 13.1 \\
WRA & 8 & 134.3 & -19.9 \\
ZALV & 9 & 84.8 & 53.9 \\ 
\end{tabular}


\section{Data and Evaluation}

The data for this problem is several hundred megabytes, and has to be
downloaded separately. The key point worth noting is that we are
providing separate training and test data sets that are generated from
the same underlying physics. The file {\tt test.data} contains the fully
labeled test data, and the file {\tt training.data} is the fully labeled
training data. In order to avoid accidental peeking, we are also
providing an unlabeled copy of the test data in {\tt test.blind}. The
evaluation function is described in detail in the following
sections. However, we are providing a simple script, {\tt evaluate.py}
to perform the evaluation.

A short data set of $100$ episodes has been included for the purpose of
evaluation on the challenge problem. A larger data set of $10000$
episodes has also been included for further research.

\subsection{File Format}

Each data file consists of a number of episodes that are separated by a
blank line. Each episode has subsections for the events, detections, and
associations that took place in one episode of $T$ seconds.
The format of the blind data is identical, however it has no events
or associations.

\begin{verbatim}
Episodes:

Events:
<longitude> <latitude> <magnitude> <time>
...
Detections:
<station number (zero-based)> <time> <azimuth> <slowness> <amplitude>
...
Assocs:
<event number (zero-based)> <detection number (zero-based)>
...

\end{verbatim}

\subsection{Evaluation}

In each episode, the predicted events are matched against the ground
truth events using min-weight max-cardinality matching. Only events that
are within $W_T=50$ seconds and $W_D=5$ degrees of each other are
considered as potential matches. The weight of a matched pairs of events
$e$ and $e'$ is,
\[\frac{|e_t - e'_t|}{W_T}  + \frac{dist(e,e')}{W_D} . \]
Given a matching, we can compute the precision, recall, and F-1 score in
each episode as well as over the entire data set. Additionally, we will
report the errors in magnitude estimates, distance, and time for the
matched events.

There is just one caveat here. Some of the ground truth events don't
have at least two associations, and thus can't be located
reasonably. These events have to be excluded from the matching and the
subsequent recall score.

The provided script {\tt evaluate.py} may be used to automatically
evaluate a solution bulletin. The format of the solution that is
expected by this script is identical to that used for the other data
files. The list of detections may be left out. It is highly recommended
though that the list of associations should not be left out because
future versions of the script may match associations as well.

The following shows the output from the script when run against a
partial bulletin with only four episodes solved.
\begin{verbatim}
./evaluate.py short_data/test.data mysolution.data
Guess data has fewer episodes than gold data!!
10 matchable events, 12 guess events, and 4 matched
Precision 33.3 % , Recall 40.0 % , F1 36.4
Time Errors mean 8.5 std 4.7
Dist Errors mean 1.4 std 0.6
Mag Errors mean 0.2 std 0.1
\end{verbatim}

\subsection{Metric and Submission}

This problem does not require a performance profile. Teams should report
the metrics output by the evaluation script at the point where their
solution halts. Teams should also report the CPU time consumed by their
solution, in milliseconds.

\begin{appendices}

\section{Spherical Geometry Functions}

\label{app-sphere}

\subsection{Azimuth}

The azimuth of location $b=(lon_2, lat_2)$ as observed from location
$a=(lon_1, lat_1)$ is given by the function $G_z(a, b) \in [0,
  360)$. Where $0$ is due north, $180$ is due south, and $270$ is due
  west. If $dlon$ = $lon_2 - lon_1$ then we define
\begin{align*}
G_z(a, b) &= [G'_z(a,b) + 360]  \mod  360 , \\
\text{where} \ G'_z(a,b) &= \text{atan2} \left(\,
\sin(dlon) \ , \ \cos(lat_1) \tan(lat_2) - \sin(lat_1) \cos(dlon)
\, \right) .
\end{align*}
See \cite{wiki-great-circle-navigation} for an explanation of $G'_z(a,b)$.

\subsection{Azimuth Difference}
The difference between two azimuths $\psi(z_1, z_2)$ measures whether
$z_2$ is clockwise from $z_1$, i.e. $\psi \in [0, 180]$ or
counter-clockwise, $\psi \in [0, -180]$.
\begin{align*}
\psi(z_1, z_2) &= \begin{dcases*}
\psi'(z_1, z_2) - 360 & if $\psi'(z_1, z_2) > 180$ \\
\psi'(z_1, z_2) & otherwise
\end{dcases*}
\intertext{where,}
\psi'(z_1, z_2) &= [z_2 - z_1 + 360] \mod 360.
\end{align*}

\subsection{Great-Circle Distance}

From \cite{wiki-great-circle-dist}:
\begin{quote} 
The great-circle distance is the shortest distance between two points on
the surface of a sphere measured along the surface of the sphere.
\end{quote}
We use the Vincenty version of the formula. If $a$ and $b$
are two points on the surface of a sphere described by longitude,
latitude pairs $(lon_1, lat_1)$ and $(lon_2, lat_2)$ respectively, and
$dlon$ is the difference in the longitudes, then
\[dist(a,b) = \text{atan2} (y, x), \]
where
\begin{align*}
y &= \sqrt{(\cos(lat_2) \sin(dlon))^2 
+ (\cos(lat_1) \sin(lat_2) - \sin(lat_1)\cos(lat_2)\cos(dlon))^2} \, 
, \, \text{and}\\
x &= {\sin(lat_1) \sin(lat_2) + \cos(lat_1) \cos(lat_2) \cos(dlon)}.
\end{align*}
Note that $atan2$ \citep{wiki-atan2} has range $[0, 180]$ degrees when the
first argument is non-negative. Hence $dist$ has range $[0, 180]$.

\section{Statistical Distributions and Functions} 

\subsection{Cauchy}

The Cauchy distribution with location $\mu$ and scale
$\theta$ has probability density
\[ \text{Cauchy}(x \ | \ \mu, \ \theta) = 
\frac{1}{\pi \theta} \ \frac{1}{ 1 + (\frac{x - \mu}{\theta})^2 }
\]
defined over all $x \in \mathbb{R}$.

\subsection{Exponential}

The Exponential distribution with location (or minimum value) $\mu$,
scale $\theta$, and maximum value $\gamma_m$ has the probability density
\[ \text{Exponential}(x \ | \ \mu, \ \theta, \ \gamma) =
\frac{1}{\theta} \
\ \frac{1}{1 - e^{-\frac{\gamma-\mu}{\theta}}} \ 
e^{-\frac{ x - \mu} {\theta}} \]
defined over all $x \in \mathbb{R}$, $x \ge \mu$, and $x < \gamma$.

\subsection{Gaussian}

The Gaussian distribution with mean $\mu$ and standard deviation
$\sigma$ has probability density
\[ \text{Gaussian}(x \ | \ \mu, \ \sigma) = \frac{1}{\sqrt{2\pi}\sigma}
\ 
e^{-\frac{(x - \mu)^2}{2\sigma^2}} \, ,\] 
defined over all $x \in \mathbb{R}$.

\subsection{Gamma}

The Gamma distribution with shape $\alpha$ and scale $\theta$ has probability
density 

\[\text{Gamma}(x \ | \ \alpha, \ \theta) = \frac{1}{\Gamma(\alpha) \ 
  \theta^\alpha} \,  x^{\alpha-1}  e^{-\frac{x}{\theta}} \, , \]
defined over all $x \in \mathbb{R}_{>0}$.

\subsection{Inverse-Gamma}

The Inverse-Gamma distribution with shape $\alpha$ and scale $\theta$
has probability density

\[\text{InvGamma}(x \ | \ \alpha, \ \theta) 
= \frac{\theta^\alpha}{\Gamma(\alpha)}
 \ x^{-\alpha-1} e^{-\frac{\theta}{x}} \, , \] defined over all $x \in
 \mathbb{R}_{>0}$.

\subsection{Logistic}

The logistic function is defined as,
\[ \text{logistic}(x) = \frac{1}{1 + e^{-x}} \ . \]

\subsection{Laplacian}

The Laplacian distribution with location $\mu$ and scale $\theta$ has
probability density
\[ \text{Laplacian}(x \ | \ \mu, \ \theta) = \frac{1}{2 \theta} \ 
e^{-\frac{|x - \mu|}{\theta}}
\, ,\]
defined over all $x \in \mathbb{R}$.


\subsection{Multi-variate Gaussian}

The Muti-variate Gaussian distribution with mean vector
$\mu \in \mathbb{R}^k$, and
covariance matrix $\Sigma  \in \mathbb{R}^{k \times k}$ has probability density

\[\text{MVarGaussian}(x \ | \ \mu, \ \Sigma ) = 
\frac{1}{\sqrt{(2 \pi)^k | \Sigma
    |}} \ 
e^{ -\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu)}
\]
defined over all $x \in \mathbb{R}^k$.

\subsection{Poisson}

The Poisson distribution with rate $\lambda$ has the probability density
\[ \text{Poisson}(n \ | \ \lambda) =  e ^ {-\lambda}  \ 
\frac{  \lambda ^ n  }{n !} \,
, \]
defined over all $n \in \mathbb{Z}_{\ge 0}$.

\subsection{Uniform}

The Uniform distribution with parameters $a, b \in \mathbb{R}$ (and $a <
b$) has the probability density
\[ \text{Uniform}(x \ | \ a, \ b) = \frac{1}{b - a} \ \mathbb{1}_{x>a}
\mathbb{1}_{x<b}\, ,\]
defined over all $x \in \mathbb{R}$.

\section{Supplied Python Scripts}

All of the supplied scripts are briefly described in the provided {\tt
  README.txt} file. Here is a more detailed description.

\subsection{generate.py}

This script first draws a sample from the hyperpriors. This sample
represents the physics of a hypothetical earth.  Next it samples a
number of episodes of seismic events and detections based on this
underlying physics. The episodes are divided into two sets, one for
training and the other for testing, and then saved. The test episodes
are further stripped of all events and associations and saved in the
blind test file. The arguments to the script are as follows.
\begin{itemize}
\item The number of episodes to sample for each of the training and test
  files.
\item The file name to save the physics.
\item The file name to save the training data.
\item The file name to save the test data.
\item The file name to save the blind test data.
\end{itemize}
The following sample output is generated when the script is run. First
we create a directory to save the data, and then invoke {\tt
  generate.py}.
\begin{lstlisting}
$> mkdir data

$> ./generate.py 100 data/physics.data data/training.data data/test.data data/test.blind
263 events generated
58.9 % events have at least two detections
316 events generated
57.9 % events have at least two detections

$> ls data
physics.data  test.blind  test.data  training.data
\end{lstlisting}
The format of the {\tt physics.data} is very simple. It simple lists
each attribute of the physics in the order specified in
Section~\ref{sec-hyperprior} followed by an equal sign and the value as
represented in Python. For example, the following are the first few
lines of this file.
\begin{lstlisting}
T = 3600
R = 6371
lambda_e = 2.44749420963e-12
mu_m = 3.0
theta_m = 4.0
gamma_m = 6.0
mu_d0 = [-15.525711059357944, -13.02388685474455, -7.7621843877940453, -11.133601793604004, -4.3077944888906963, -6.4595999202909073, -6.0893863902811285, -11.5243613488537, -10.974148036814023, -13.319638578401111]
\end{lstlisting}

\subsection{util.py}
This script has a number of utilities that are described below.
\begin{itemize}
\item {\tt compute\_travel\_time} is the function $I_T$.
\item {\tt compute\_slowness} is the function $I_S$.
\item {\tt invert\_slowness} is the function $I_S^{-1}$.
\item {\tt write\_namedtuple} writes out a Python {\em named tuple} to
  file. This is used to write the physics out to file.
\item {\tt read\_namedtuple} clearly reads a Python {\em named tuple}
  from file. This function is handy to read the physics data.
\item {\tt write\_episodes} writes out a list of episodes to file.
\item {\tt write\_single\_episode} writes out a single episode to an
  existing file stream.
\item {\tt read\_episodes} reads a list of episodes from a file.
\item {\tt iterate\_episodes} sets up a Python iterator for iterating
  episodes in a file.
\item {\tt compute\_distance} is the $dist$ function.
\item {\tt compute\_azimuth} is the $G_z$ function.
\item {\tt invert\_dist\_azimuth} computes a location by traveling a
  certain amount of distance in a given azimuth from a fixed
  location. This problem is also called {\em reckoning}. The use of this
  function is critical to the sample solver.
\item {\tt compute\_degdiff} is the $\psi$ function.
\item {\tt mvar\_norm\_fit} for fitting data to a multivariate normal.
\item {\tt mvar\_norm\_sample} for sampling from a multivariate
  normal. This function is used in {\tt generate.py} to generate the
  detection probability coefficient, for example.
\end{itemize}

\subsection{solve.py}

This sample solver is in a very rudimentary stage as of this
writing. This version cheats by looking directly at the physics rather
than learning the physics from the training data. The arguments to the
script are as follows.
\begin{itemize}
\item The name of the physics data file.
\item The name of the blind data file.
\item The name of the file where the solutions are to be written out.
\end{itemize}
Here is a sample
command to run this script.
\begin{verbatim}
$> ./solve.py data/physics.data data/test.blind data/test.solution
\end{verbatim}
The script keeps printing out to the terminal the events that it has
located while populating the solution file as well. The solution file is
flushed at the end of each episode.

A brief summary of the algorithm is as follows. For each detection we
invert the slowness to get a distance estimate. Next invert the distance
estimate and azimuth to get a potential event location, and finally the
event time and magnitude is computed from the location. Now, repeat this
process with perturbed slowness and azimuth values to get a number of
candidates events from each detection. A large number of candidates, roughly a
hundred from each detection, is thus generated. A candidate is assigned
a score by attempting to associate the best set of detections from the
available pool. An event score is the log likelihood ratio of the
associated detections being generated by the event versus the same
detections being generated by noise.

Once a best candidate is identified, its associated detections are
removed from the pool and the process is repeated. Once the score of a
candidate event is below a threshold the process stops.

Of course, this algorithm makes a number of simplifications. For example,
it ignores the event prior and doesn't consider splitting or merging
events. Please refer to \cite{Arora2013} for the actual deployed
algorithm.

\subsection{evaluate.py}
This script takes only two arguments. The data file with the correct
bulletin, the so-called {\em gold} data, and the guess data file. The
guess file could have fewer episodes than the gold data. In this case
only the episodes in the guess data file are evaluated. Here is a sample
output from the script.
\begin{lstlisting}
$> ./evaluate.py data/test.data data/test.solution 
Guess data has fewer episodes than gold data!!
115 matchable events, 147 guess events, and 84 matched
Precision 57.1 % , Recall 73.0 % , F1 64.1
Time Errors mean 6.7 std 5.6
Dist Errors mean 1.4 std 0.9
Mag Errors mean 0.2 std 0.1
\end{lstlisting}

\subsection{mwmatching.py}

This utlity script implements a max-weight max cardinality matching
algorithm that is used for the evaluation.

\end{appendices}

\bibliographystyle{chicagoa}
\bibliography{seismic}

\end{document}

